### Introduzione
---
Lunedi: giornata riunione di allineamento (riunione per dire dove siamo arrivati e cosa bisogna fare)

OWEB: strumento interno aziendale dove si inserisce quello che si e' fatto il giorno stesso. Fatto in .NET framework 4.6 VB
	diviso in impianto per Sigemi, production, testing (anche per la fatturazione) e adesso anche per HSC.
OWEB: https://oweb.sigemi.it/xts/?appname=sigemi

Pass Z4U: .BSX437sXg.dV2H

RDP 34.59.76.254 remote desktop protocol, per la connessione con un'altra macchina con interfaccia grafica, ma serve la autenticazione.

macchina in cloud di sviluppo con strumenti per lo sviluppo
### ETL e DataPeople
---
ETL extract transform load (di dati)

DataPeople: 
dati arrivano da 3 fonti principali : adHoc infinity, workflow, cassopea (CRM)
i dati servono per il controllo di gestione per vedere i costi e i ricavi, per farli vedere ai clienti per la loro profittabilita'

data warehouse, utilizzando big query, vengono stipati tutti i dati necessari per analizzare tutti i dati per datapeople.
noi forniammo un servizio ETL (open ETL) con 3 tipi:
extract -> prendere qualcosa dalle 3 fonti o altrove
transform -> raramente lo fanno
load -> prendere i dati e caricarli su BQ.

i 3 tipi sono industrializzati:
- SQL to BQ una query sql che viene eseguita su sql server e i due applicativi zucchetti adhoc workflow e i dati vengono messi su BQ, l'unico passaggio e' la conversione di tipi
- REST to BQ - quest'anno si ha cassiopea, che gira i nostri impianti, non ha un database, ma viene fornita un API a cui viene fatta una chiamata REST bisogna autenticarsi con un codice di esportazione. in questo caso c'e' una parte di transformation molto piu' lunga con ZCS perche' l'output e' in JSON.
- SFTP to BQ - l'origine dei dati e' differente, e' una cartella dove il cliente carica il JSON o il CSV, e ogni tot minuti viene pollato, transformato e poi caricato. Oppure quando viene caricato uno storico per tutte le altre fonti. (solitamente le ultime 2 settimane di storico) e la tabella di BQ viene overwritten.

L'ultima parte che e' il caricamento del dato in una tabella. Esse hanno un chronjob, ovvero hanno una schedule per avviare un azione. Una volta avviato i dati sono stati parsati.

BQ -> data warehouse, e' un olab, un database analitico, ovvero e' ottimizzato per la lettura ma non e' fatto per la scrittura troppi dati, quando andiamo a fare una insert in una tabella, con l'API viene generato una streaming buffer, dove i dati sono disponibili prima che vengano caricati nella tabella ma non sono modificabili (eta. 90m).

Nella seconda immagine, si crea invece una tabella nuova temporanea, da cui viene eseguita una merge per utilizzare piu' volte le ETL sulla stessa tabella.

trattiamo i dati hrz, zcs, fsistemi. I flussi per ottimizzare i costi, si fa prima una controllo dove si controlla se prima i dati sono tutti in una sola sorgente, oppure dove le aziende hanno i propri db.

con una sola sorgente sono i flussi comuni per esempio di adhoc, dove si tirano fuori i dati di piu' aziende con 2 query e una union. Per fare in modo che non si mischino, si ha un campo "origine" per flaggare di quale aziende sono i dati.

nei flussi non comuni come su workflow, ci sono 1 o piu' flussi per la setta azienda ma che puntano alla stessa tabella, e per riconoscere i dati si usa sempre il camp "origine" per distinguere i dati.

Con cassiopea invece tutte le rest avranno un dataset dedicato (tabella crm) e per ogni azienda nuova si aggiungera' un dataset con una tabella crm.